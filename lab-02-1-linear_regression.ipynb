{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-02-1 linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reprducibilty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and Y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find value for W and b to compute y_data = x_data * W + b  \n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let's TensorFlow figure it out \n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서플로우를 실행시키면 , 텐서플로우가 학습하는 과정에서 변경 시키는 variable이다 :  \n",
    "(trainable한 variable이라 함)\n",
    "tf.variable을 만들때는 일단 tf.Variable의 shape이 어떤가를 정의하고 값을 줌  \n",
    "이경우에는 H(x)=Wx+b\n",
    "이경우엔 W,b를 모르니까 랜덤한 값을 줌\n",
    "tf.random_normal([1]) <-shape을 값이 하나인(rank가 1인)  1차원 array를 선언 함\n",
    "w,b 선언\n",
    "Hypothsis를 W,b의 노드(텐스)를 가지고 구성함 => graph를 빌딩하는 과정에서 hypothesis 노드가 됨 (H(x)=Wx+b)\n",
    "\n",
    "#cost(W,b) 는 어떻게 할 것인가\n",
    "그대로 tf에있는 함수를 이용해 써주면 됨\n",
    "\n",
    "#reduce_mean()은 텐서가 주어질때 그부분을 평균내줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable 이라는 node\n",
    "기존의 프로그래밍 변수 와는 다름\n",
    "\n",
    "W와 b에 대한 값들을 정의해야함: 텐서플로우에서 Variable이라는 노드로 정의\n",
    "\n",
    "텐서플로우가 사용하는 variable이고 텐서가 자체적으로 변경시키는 값, = trainable한 variable이다로 봐도 됨\n",
    "\n",
    "tensorflow가 학습하는 과정에서 자기가 변경 시킨다는 말\n",
    "\n",
    "#### Vairable만들때\n",
    "variable의 tensorflow의 shape이 어떻게 되는가 정의하고 값을 줌\n",
    "\n",
    "W,b의 값 모를때 보통 렌덤한 값으로 줌 : tf.random_normal([shape을 줌]) #[1] 랭크가 1=값이 하나인 1차원 array\n",
    "\n",
    "이렇게 w,b를 만들어 선언 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Out hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis를 W와 b의 노드 , W b 텐스를 가지고 구성하는데 간단히 아래와 같이 *hypotheis노드를 만들어 줬음 (그래프를 빌딩하는 과정에서)\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "cost는 어떻게 할 것인가\n",
    "\n",
    "=> cost(w,b)=1/m......식을 그대로 텐서플로우의 함수로 구현\n",
    "\n",
    "##### \n",
    "tf.square : 텐서플로어 스퀘어함수\n",
    "reduce_mean : 어떤 텐서가 주어질때 그것을 평균 내주는 함수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost 가 주어진 다음 작업은 \n",
    "cost를 minimize하는 것-> tensorflow에 여러가지 방법 있는데, 그중에 하나인 Gradient Descent라는 것 사용 (지금은 그냥 매직이라 생각하면 됨) \n",
    "optimizer를 정의하고 \n",
    "optimizer에 minimize란 함수를 호출하면서 무엇을 미니마이즈 핡것인가 에서 cost를 미니마이즈 하라 주면 \n",
    "이것이 우리가 선언한 tensorflow variable W와 b를 조정해서 자기가 스스로 minimize하게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기까지 그래프가 구현 됨 ####\n",
    "실행하려면 session을 만들고 \n",
    "variable W,b 텐서 변수 사용했으므료\n",
    "반드시 (tf.global_variables_initializer())를 실행시켜줘야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 매직이였던 minimize하기 위한 작업의 노드 이름을 train이라고 줌\n",
    "그래프 상에서 노드의 이름이 train이 됨\n",
    "  #train = optimizer.minimize(cost)\n",
    " train을 실행 시켜야 cost를 minimize해서 W,b로 연결이 됨 => see.run(train)으로 실행시킨다.\n",
    " 2000번 스탭을 도는데\n",
    " 이때마다 출력 하기 힘드니 20번에 한번씩, 출력, cost값, w의 그래프에 있는 노드 값을 실행시킴 (그래프의 노드 값을 evaluation해서 가져온단 멀), b의 값도 잘 나오는지 볼 수 있음\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.82329 [ 2.12867713] [-0.85235667]\n",
      "200 0.0699668 [ 1.30721486] [-0.69837117]\n",
      "400 0.0267167 [ 1.18983996] [-0.4315508]\n",
      "600 0.0102017 [ 1.11730957] [-0.26667204]\n",
      "800 0.00389553 [ 1.07249022] [-0.16478711]\n",
      "1000 0.0014875 [ 1.04479456] [-0.10182849]\n",
      "1200 0.000567998 [ 1.02768016] [-0.06292368]\n",
      "1400 0.000216891 [ 1.01710474] [-0.03888312]\n",
      "1600 8.28196e-05 [ 1.01056981] [-0.02402747]\n",
      "1800 3.16242e-05 [ 1.00653136] [-0.01484741]\n",
      "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n"
     ]
    }
   ],
   "source": [
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learns best fit W:[ 1.], b:[  0.]\n",
    "\n",
    "```\n",
    "0 3.51465 [-0.03875808] [ 0.40560108]\n",
    "200 0.0286106 [ 0.80354649] [ 0.44658491]\n",
    "400 0.010925 [ 0.87860364] [ 0.27596244]\n",
    "600 0.00417169 [ 0.92498434] [ 0.17052816]\n",
    "800 0.00159296 [ 0.95364493] [ 0.10537602]\n",
    "1000 0.000608266 [ 0.97135544] [ 0.06511588]\n",
    "1200 0.000232263 [ 0.98229945] [ 0.04023758]\n",
    "1400 8.86917e-05 [ 0.98906201] [ 0.02486444]\n",
    "1600 3.38653e-05 [ 0.99324113] [ 0.01536462]\n",
    "1800 1.2932e-05 [ 0.99582338] [ 0.00949448]\n",
    "2000 4.9383e-06 [ 0.99741906] [ 0.00586706]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train 노드-cost node- hypothesis node-w-b의 형태로 연결 된 그래프 안에서\n",
    "train을 실행시킨단 의미는\n",
    "그래프를 따라 들어ㅏ가서 결국은 w와 b를 조정할 수 있게 다 연결된 그래프라고 보면 됨\n",
    "이런 그래프 빌딩한 다음에 train을여기서 실행시키면\n",
    "이떄 여기서 학습이 일어나고\n",
    "학습 후에 \n",
    "cost,w, b의 값을 보는 것\n",
    "\n",
    "이런식으로 자동적으로 optimizer를 실행시키면 train이 일어나서 w와 b의 값을 tensorflow가 조정하게 됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Place holders 개념\n",
    "직접 값을 주지 않고 placeholder라는 이름으로 노드를 준다음에 feed_dict로 값을 넘겨줌\n",
    "\n",
    "placehold사용해 linear regression실행시키기\n",
    "실제 x,y값 주지 않고 \n",
    "tf.placeholder란 이름으로 주고 => shape도 None으로 줄 수 있음 shpe=[None]\n",
    "feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정리\n",
    "텐서플로우를 이용해서 한 것 \n",
    "1. H(x)와 cost에 해당되는 그래프를 만듬\n",
    "2. 만들어진 모델 사용 위해 feed_dict라는 방법을 이요해 \n",
    "원하는 학습 x,y데이터 주면서 그래프를 실행 시켰고\n",
    "3.결과로 내부적으로 w,b가 update됐고 필요할때는 return value시켜 출력함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
